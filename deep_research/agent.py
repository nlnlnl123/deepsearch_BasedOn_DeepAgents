"""Research Agent - Standalone script for LangGraph deployment.

This module creates a deep research agent with custom tools and prompts
for conducting web research with strategic thinking and context management.
"""

import asyncio
import os
from datetime import datetime
from pathlib import Path

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, FilesystemBackend, StoreBackend
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.store.memory import InMemoryStore

from research_agent.prompts import (  # research_agent/prompts.py
    RESEARCH_WORKFLOW_INSTRUCTIONS,
    RESEARCHER_INSTRUCTIONS,
    SUBAGENT_DELEGATION_INSTRUCTIONS,
)
from research_agent.tools import tavily_search, think_tool  # research_agent/tools.py

# Limits
max_concurrent_research_units = 3
max_researcher_iterations = 3

# Get current date
current_date = datetime.now().strftime("%Y-%m-%d")

# å®Œæ•´çš„ç ”ç©¶å·¥ä½œæµç¨‹æŒ‡ä»¤ï¼ŒåŒ…å«å¼•ç”¨æ–‡çŒ®è¦æ±‚ è¿™é‡Œæç¤ºè¯ä¸€è¡Œéƒ½ä¸èƒ½å°‘ï¼ï¼ï¼
FULL_RESEARCH_INSTRUCTIONS = f"""{RESEARCH_WORKFLOW_INSTRUCTIONS}

## File Writing Requirements

**CRITICAL**: You MUST use the write_file tool to save files. Do NOT just output text.

1. **Save research request**: Use write_file("/research_request.md", content) to save the user's research question
2. **Save final report**: Use write_file("/final_report.md", content) to save your comprehensive report
**IMPORTANT**:
- Use absolute paths starting with "/" (e.g., "/research_request.md", "/final_report.md")
- The final report MUST include proper citations in [1], [2], [3] format
- The final report MUST end with a ### Sources section listing all numbered sources
- Each source should be formatted as: [1] Source Title: URL
- Do NOT just output text - you MUST use the write_file tool!

{SUBAGENT_DELEGATION_INSTRUCTIONS.format(
    max_concurrent_research_units=max_concurrent_research_units,
    max_researcher_iterations=max_researcher_iterations,
)}
"""

def make_backend(runtime):
    """åˆ›å»ºå¤åˆå­˜å‚¨åç«¯."""
    base_dir = Path(__file__).parent
    research_dir = base_dir / "research_outputs"
    research_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ Backend configured. Files will be saved to: {research_dir}")
    print(f"ğŸ“ Absolute path: {research_dir.absolute()}")
    
    # ä½¿ç”¨ virtual_mode=Trueï¼Œè¿™æ ·ç»å¯¹è·¯å¾„ä¼šè§£æåˆ° root_dir ä¸‹
    fs_backend = FilesystemBackend(
        root_dir=str(research_dir.absolute()),
        virtual_mode=True  # æ”¹ä¸º Trueï¼Œè¿™æ · /research_request.md ä¼šè§£æåˆ° root_dir/research_request.md
    )
    
    return CompositeBackend(
        default=fs_backend,
        routes={"/memories/": StoreBackend(runtime)}
    )
# åˆ›å»ºæ¨¡å‹ï¼ˆæ·»åŠ é‡è¯•é…ç½®ï¼‰
model = ChatOpenAI(
    base_url=os.getenv("DASHSCOPE_API_BASE"),
    api_key=os.environ.get("DASHSCOPE_API_KEY"),
    model="qwen-max-2025-01-25", # é€‰å¥½æ¨¡å‹å¾ˆå…³é”®ï¼Œå·®æ¨¡å‹ä¼šfail
    timeout=60.0,
    max_retries=3,
)

# æ„å»ºresearch sub-agentå­—å…¸
research_sub_agent = {
    "name": "research-agent",
    "description": "Delegate research to the sub-agent researcher. Only give this researcher one topic at a time.",
    "system_prompt": RESEARCHER_INSTRUCTIONS.format(date=current_date) ,
    "tools": [tavily_search, think_tool],
    "model": model
}



# åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨
store = InMemoryStore()

# Create the agent with official backends support
agent = create_deep_agent(
    model=model,
    tools=[tavily_search, think_tool],  # ä¸»ä»£ç†ä¹Ÿæ‹¥æœ‰æœç´¢å·¥å…·ï¼Œä½†ä¼šä¼˜å…ˆä½¿ç”¨å­ä»£ç†
    backend=make_backend,
    system_prompt=FULL_RESEARCH_INSTRUCTIONS,
    subagents=[research_sub_agent],
    store=store,
    debug=True
)

async def run_with_retry(agent, messages, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„ agent æ‰§è¡Œ."""
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ Attempt {attempt + 1}/{max_retries}...")  # noqa: T201
            result = await agent.ainvoke({"messages": messages})
            return result
        except Exception as e:
            print(f"âŒ Attempt {attempt + 1} failed: {e}")  # noqa: T201
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2)
    return None

if __name__ == "__main__":
    print("ğŸš€ Starting research agent...")  # noqa: T201
    print(f"ğŸ“ Research topic: 'context engineering approaches used to build AI agents'")  # noqa: F541, T201
    
    research_output_dir = Path(__file__).parent / "research_outputs"
    research_output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†ä¹‹å‰çš„æ–‡ä»¶ï¼ˆå› ä¸º write_file ä¸è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
    for file in research_output_dir.glob("*.md"):
        file.unlink()
        print(f"ğŸ§¹ Cleaned up: {file.name}")  # noqa: T201
    
    try:
        # è¿è¡Œ agent
        messages = [
            {
                "role": "user",
                "content": "research context engineering approaches used to build AI agents",
            }
        ]
        
        result = asyncio.run(run_with_retry(agent, messages))
        
        print("\nğŸ“‹ Agent execution completed. Messages:")
        print("=" * 80)
        
        # ç»Ÿè®¡æ¶ˆæ¯ç±»å‹
        human_count = ai_count = tool_count = 0
        write_file_calls = []
        
        for msg in result["messages"]:
            if isinstance(msg, HumanMessage):
                human_count += 1
                print(f"\nğŸ‘¤ Human message {human_count}:")
                print(f"   {msg.content}")
            elif isinstance(msg, AIMessage):
                ai_count += 1
                print(f"\nğŸ¤– AI message {ai_count}:")
                if hasattr(msg, 'content'):
                    content = msg.content
                    print(f"   Content: {content[:200]}..." if len(content) > 200 else f"   Content: {content}")
                # æ£€æŸ¥å·¥å…·è°ƒç”¨
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    print(f"   Tool calls: {len(msg.tool_calls)}")
                    for i, tc in enumerate(msg.tool_calls):
                        tool_name = tc['name']
                        print(f"   [{i+1}] Tool: {tool_name}")
                        if tool_name == 'write_file':
                            write_file_calls.append(tc)
                            print(f"       Arguments: {tc.get('args', {})}")
                        elif tool_name == 'task':
                            print(f"       âœ… Task tool called! Arguments: {tc.get('args', {})}")
                        else:
                            print(f"       Arguments: {tc.get('args', {})}")
                else:
                    print(f"   No tool calls")
            elif isinstance(msg, ToolMessage):
                tool_count += 1
                print(f"\nğŸ”§ Tool message {tool_count}:")
                if hasattr(msg, 'name'):
                    tool_name = msg.name
                    print(f"   Tool: {tool_name}")
                    print(f"   Content: {msg.content[:200]}...")
                else:
                    print(f"   Content: {msg.content[:200]}...")
        
        print(f"\nğŸ“Š Summary:")
        print(f"   Human messages: {human_count}")
        print(f"   AI messages: {ai_count}")
        print(f"   Tool messages: {tool_count}")
        print(f"   write_file calls: {len(write_file_calls)}")
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–‡ä»¶
        print("\n" + "=" * 80)
        print("ğŸ“ Checking for generated files:")
        
        research_request_path = research_output_dir / "research_request.md"
        final_report_path = research_output_dir / "final_report.md"
        
        if research_request_path.exists():
            print(f"âœ… Generated research_request.md at: {research_request_path}")
            with open(research_request_path, encoding='utf-8') as f:
                content = f.read()
                print(f"   Content preview: {content[:150]}...")
        else:
            print("âŒ research_request.md NOT generated")
            print("   Expected at: {research_request_path}")
        
        if final_report_path.exists():
            print(f"âœ… Generated final_report.md at: {final_report_path}")
            with open(final_report_path, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"   Content preview: {content[:150]}...")
        else:
            print(f"âŒ final_report.md NOT generated")
            print(f"   Expected at: {final_report_path}")
        
        # åˆ—å‡º research_outputs ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        print(f"\nğŸ“ All files in research_outputs directory ({research_output_dir}):")
        if research_output_dir.exists():
            files = list(research_output_dir.iterdir())
            if files:
                for file in files:
                    print(f"   - {file.name} ({file.stat().st_size} bytes)")
            else:
                print("   Directory is empty!")
        else:
            print("   Directory does not exist!")
            
        # å¦‚æœæ²¡æœ‰ç”Ÿæˆæ–‡ä»¶ï¼Œæä¾›è¯Šæ–­ä¿¡æ¯
        if not research_request_path.exists() or not final_report_path.exists():
            print("\nâš ï¸  DIAGNOSIS:")
            print("   Files were not generated. Possible reasons:")
            print("   1. Agent did not call write_file tool")
            print("   2. Tool call failed silently")
            print("   3. Backend configuration issue")
            print(f"   4. write_file calls detected: {len(write_file_calls)}")
            
    except Exception as e:
        print(f"âŒ Error during agent execution: {e}")
        import traceback
        traceback.print_exc()